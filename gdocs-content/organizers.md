# Organizing Committee

## Neel Nanda (Contact Person)
**Affiliation:** Senior Research Scientist, Google DeepMind  
**Email:** neurips2025@mechinterpworkshop.com

Neel Nanda is a Senior Research Scientist at Google DeepMind where he leads the mechanistic interpretability team. His team's recent work has focused on sparse autoencoders, introducing the state of the art JumpReLU architecture, and the widely used Gemma Scope suite of open source SAEs.

## Martin Wattenberg
**Affiliation:** Professor, Harvard University & Research Scientist, Google DeepMind

Martin Wattenberg is a professor of computer science at Harvard University and a research scientist at Google DeepMind. His expertise encompasses AI interpretability, human-AI interaction, and data visualization.

## Sarah Wiegreffe
**Affiliation:** Assistant Professor, University of Maryland & Postdoc, Allen Institute for AI

Sarah Wiegreffe is an incoming assistant professor at the University of Maryland and a current postdoc at the Allen Institute for AI (Ai2) and the University of Washington. She has worked on the explainability and interpretability of neural networks for NLP since 2017.

## Atticus Geiger
**Affiliation:** Lead, Pr(Ai)Â²R Group

Atticus finished his PhD thesis Uncovering and Inducing Causal Structure in Deep Learning Models at Stanford in 2023. Since then, he has led the Pr(Ai)2R Group, a small non-profit mechanistic interpretability research lab.

## Julius Adebayo
**Affiliation:** Founder and Researcher, Guide Labs

Julius Adebayo is a founder and researcher at Guide Labs. He is interested in building large-scale models that are engineered to be modular, steerable, and easy for humans to understand by enforcing interpretability constraints as part of the learning process.

## Kayo Yin
**Affiliation:** 3rd year PhD student, UC Berkeley

Kayo Yin is a 3rd year PhD student at UC Berkeley. She works on LLM interpretability and NLP for signed languages. Her research has been recognized by the ACL 2023 Best Resource Paper award and other honors.

## Fazl Barez
**Affiliation:** Senior Research Fellow, University of Oxford

Fazl Barez is a Senior Research Fellow at University of Oxford, where he works on topics related to Interpretability, AI safety, policy and governance.

## Lawrence Chan
**Affiliation:** Researcher, METR

Lawrence Chan is a researcher working on understanding the behavior of language models by either their internal components or via extensive human evaluations.