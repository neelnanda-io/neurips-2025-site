content: "## Workshop Goals\nThe mechanistic interpretability field benefits from\
  \ a rich diversity of approaches\u2014from rigorous mathematical analysis to large-scale\
  \ empirical studies, from reverse-engineering a model via bottom-up circuit analysis,\
  \ to assisting behavioral analysis via top-down analysis of model representations.\
  \ But all are unified by the belief that there is meaning and structure to be found\
  \ inside neural networks, and that this is worth studying. \n\nThis diversity reflects\
  \ the field's breadth and the many valid paths toward understanding neural networks.\
  \ But those in these different sub-communities often lack natural venues to meet.\
  \ Our workshop aims to: \n* **Showcase cutting-edge research** across all approaches\
  \ to mechanistic interpretability\n* **Foster cross-pollination** between different\
  \ methodological traditions\n* **Identify convergent insights** emerging from diverse\
  \ research programs\n* **Build understanding** between different perspectives, research\
  \ communities, and terminologies\n* **Welcome newcomers** by providing clear entry\
  \ points into the field\n\nWe hope to explore points of active debate in the field\
  \ including: \n* How researchers should prioritise between gathering evidence via\
  \ rigorous qualitative analysis vs performance on benchmarks/real-world tasks\n\
  * What are the implications of new paradigms like reasoning models for the field\u2019\
  s priorities?\n* Whether the field\u2019s north star should be complete reverse\
  \ engineering, achieving high-level understanding, or something else entirely\n\
<<<<<<< HEAD
  * How reliable or useful are popular methods such as [sparse](https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&sa=D&source=editors&ust=1752365984390039&usg=AOvVaw1-4MtlGyyu2EF9yFT_Zc2O)\
  \ [autoencoders](https://www.google.com/url?q=https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html&sa=D&source=editors&ust=1752365984390224&usg=AOvVaw29Mf458A2pX4XbzltQ61tk),\
  \ and how much should we [prioritize](https://www.google.com/url?q=https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9&sa=D&source=editors&ust=1752365984390430&usg=AOvVaw3tTM0OjYeD5n-ytJasxxpv)\
  \ them compared to other research directions?\n* The relative merits of a perspective\
  \ of curiosity driven basic science vs working towards specific goals\n* How important\
  \ are unsupervised techniques with the potential to surprise us, such as [transcoders](https://www.google.com/url?q=https://transformer-circuits.pub/2025/attribution-graphs/biology.html&sa=D&source=editors&ust=1752365984390860&usg=AOvVaw0wl5czF7UCueLtd7c2D7fJ),\
  \ compared to simple supervised techniques such as [probing](https://www.google.com/url?q=https://arxiv.org/abs/2102.12452&sa=D&source=editors&ust=1752365984391036&usg=AOvVaw3Nwy7_sA_zjDlxsvslZLoe).\n\
=======
  * How reliable or useful are popular methods such as [sparse](https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&sa=D&source=editors&ust=1752365868588340&usg=AOvVaw3u1Xn_frVPVNvCxbRNlcUT)\
  \ [autoencoders](https://www.google.com/url?q=https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html&sa=D&source=editors&ust=1752365868588456&usg=AOvVaw2tqe2OEDNT5otpar4jtHIH),\
  \ and how much should we [prioritize](https://www.google.com/url?q=https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9&sa=D&source=editors&ust=1752365868588583&usg=AOvVaw33d4AWaK08HC3MOpBLBSiD)\
  \ them compared to other research directions?\n* The relative merits of a perspective\
  \ of curiosity driven basic science vs working towards specific goals\n* How important\
  \ are unsupervised techniques with the potential to surprise us, such as [transcoders](https://www.google.com/url?q=https://transformer-circuits.pub/2025/attribution-graphs/biology.html&sa=D&source=editors&ust=1752365868588857&usg=AOvVaw2YiOtjelXK1WWPomu_mxPo),\
  \ compared to simple supervised techniques such as [probing](https://www.google.com/url?q=https://arxiv.org/abs/2102.12452&sa=D&source=editors&ust=1752365868588954&usg=AOvVaw3NJzP68NnZFINwKSGDzl80).\n\
>>>>>>> 13b2c72 (x)
  \nIn this workshop, we hope to bring together researchers from across these many\
  \ perspectives and communities\u2014along with skeptics, experts in adjacent fields,\
  \ and those simply curious to learn more\u2014to facilitate healthy discussion and\
  \ move towards a greater mutual understanding as a field. \n\nThrough our [call\
<<<<<<< HEAD
  \ for papers](https://www.google.com/url?q=https://mechinterpworkshop.com/cfp/&sa=D&source=editors&ust=1752365984391601&usg=AOvVaw0pOK2WmuvsZa0HCM4vRW3D),\
=======
  \ for papers](https://www.google.com/url?q=https://mechinterpworkshop.com/cfp/&sa=D&source=editors&ust=1752365868589333&usg=AOvVaw1V68c9SuYco1vkVDZmvUEY),\
>>>>>>> 13b2c72 (x)
  \ we hope to facilitate the sharing of work in this fast-moving field, across all\
  \ of these axes, and especially work that helps to bridge these gaps. **We welcome\
  \ any submissions that seek to further our ability to use the internals of models\
  \ to achieve understanding, regardless of how unconventional the approach may be.**\
<<<<<<< HEAD
  \ Please see the [call for papers page](https://www.google.com/url?q=https://mechinterpworkshop.com/cfp/&sa=D&source=editors&ust=1752365984392104&usg=AOvVaw08c_XzjcXHcYqJJ5f1GGY5)\
=======
  \ Please see the [call for papers page](https://www.google.com/url?q=https://mechinterpworkshop.com/cfp/&sa=D&source=editors&ust=1752365868589629&usg=AOvVaw20TOZMtQsiVtu-BEVxolcu)\
>>>>>>> 13b2c72 (x)
  \ for further details and particular topics of interest. \n\nWe welcome attendees\
  \ from all backgrounds, regardless of your prior research experience or if you have\
  \ work published at this workshop. Note that while you do not need to be registered\
  \ for the NeurIPS main conference to attend this workshop, you do need to be registered\
  \ for the NeurIPS workshop track. No further registration needed, seating is first-come\
  \ first-served. \n## Learning More\nHere are some resources you may find useful\
  \ for learning more about the mechanistic interpretability field and performing\
  \ research: \n* We recommend starting with the review paper <span class=\"open-problems-text\"\
<<<<<<< HEAD
  >[Open](https://www.google.com/url?q=https://arxiv.org/abs/2501.16496&sa=D&source=editors&ust=1752365984393453&usg=AOvVaw3GfYa1SMoa8YsLf-Mg02Vd)\
  \ Problems in Mechanistic Interpretability</span> for an overview of the field\n\
  * [Ferrando et al](https://www.google.com/url?q=https://arxiv.org/abs/2405.00208&sa=D&source=editors&ust=1752365984393708&usg=AOvVaw0gmJnyuYxVyK6c4hP8_COz)\
  \ is a good primer on the key techniques of the field\n* The [ARENA coding tutorials](https://www.google.com/url?q=https://arena-chapter1-transformer-interp.streamlit.app/&sa=D&source=editors&ust=1752365984394033&usg=AOvVaw3ifg4FD2Uxu0PSOxxBmyBo)\
  \ are a great place to learn how to implement these techniques in practice\n\nResources\
  \ for doing research \n* Popular libraries include: [TransformerLens](https://www.google.com/url?q=https://github.com/TransformerLensOrg/TransformerLens&sa=D&source=editors&ust=1752365984394565&usg=AOvVaw3YcvprhIN17H4h6un7nIzW)\
  \ (PyTorch, best for <=9B models), [nnsight](https://www.google.com/url?q=https://github.com/ndif-team/nnsight&sa=D&source=editors&ust=1752365984394711&usg=AOvVaw2oxdJPzNw0KTm2950Ozxst)\
  \ (PyTorch, more flexible and scales better), [Penzai](https://www.google.com/url?q=https://github.com/google-deepmind/penzai&sa=D&source=editors&ust=1752365984394848&usg=AOvVaw2kmIO4CGr8up0wIZIEGdl7)\
  \ (Jax)\n* The [Mechanistic Interpretability Benchmark](https://www.google.com/url?q=https://mib-bench.github.io/&sa=D&source=editors&ust=1752365984395114&usg=AOvVaw1S98Ty0DANkZ36LIYKIB82)\n\
  * The [Gemma Scope Sparse Autoencoders](https://www.google.com/url?q=https://arxiv.org/abs/2408.05147&sa=D&source=editors&ust=1752365984395377&usg=AOvVaw2AwXLG3Gm05dy8GMyyQrlV)\
  \ ([interactive tutorial](https://www.google.com/url?q=http://neuronpedia.org/gemma-scope&sa=D&source=editors&ust=1752365984395522&usg=AOvVaw0Q0CBuspcCA3d2Wq6KF-ii))\n\
  \nRelevant online communities: \n* [Open Source Mechanistic Interpretability Slack](https://www.google.com/url?q=http://neelnanda.io/osmi-slack-invite&sa=D&source=editors&ust=1752365984395848&usg=AOvVaw09ce3kmEvAzwkFHwQFWjrp)\n\
  * [Mechanistic Interpretability Discord](https://www.google.com/url?q=https://discord.gg/ysVfhCfCKw&sa=D&source=editors&ust=1752365984396041&usg=AOvVaw1s9TYc_rTH0z1BGqqUNDYM)\n\
  * [Eleuther Discord](https://www.google.com/url?q=https://discord.gg/nHS4YxmfeM&sa=D&source=editors&ust=1752365984396153&usg=AOvVaw2z9vsBZaX8iO6GM4gr-5U5)"
=======
  >[Open](https://www.google.com/url?q=https://arxiv.org/abs/2501.16496&sa=D&source=editors&ust=1752365868590213&usg=AOvVaw2lVH2miVNOOGwSW86mHXua)\
  \ Problems in Mechanistic Interpretability</span> for an overview of the field\n\
  * [Ferrando et al](https://www.google.com/url?q=https://arxiv.org/abs/2405.00208&sa=D&source=editors&ust=1752365868590302&usg=AOvVaw0wa9BTHKSKCCkujT-7Nu1_)\
  \ is a good primer on the key techniques of the field\n* The [ARENA coding tutorials](https://www.google.com/url?q=https://arena-chapter1-transformer-interp.streamlit.app/&sa=D&source=editors&ust=1752365868590433&usg=AOvVaw2g99_aNR6xXo1cT_avdvaw)\
  \ are a great place to learn how to implement these techniques in practice\n\nResources\
  \ for doing research \n* Popular libraries include: [TransformerLens](https://www.google.com/url?q=https://github.com/TransformerLensOrg/TransformerLens&sa=D&source=editors&ust=1752365868590638&usg=AOvVaw3auRuCjly2bm9jUT_KZ9t1)\
  \ (PyTorch, best for <=9B models), [nnsight](https://www.google.com/url?q=https://github.com/ndif-team/nnsight&sa=D&source=editors&ust=1752365868590717&usg=AOvVaw0QiXgqDARDWRJqT_FeJMDD)\
  \ (PyTorch, more flexible and scales better), [Penzai](https://www.google.com/url?q=https://github.com/google-deepmind/penzai&sa=D&source=editors&ust=1752365868590805&usg=AOvVaw2HHStzki4LspTHLIMVLkIg)\
  \ (Jax)\n* The [Mechanistic Interpretability Benchmark](https://www.google.com/url?q=https://mib-bench.github.io/&sa=D&source=editors&ust=1752365868590913&usg=AOvVaw3s7ckxg83qUp4T32Nor41i)\n\
  * The [Gemma Scope Sparse Autoencoders](https://www.google.com/url?q=https://arxiv.org/abs/2408.05147&sa=D&source=editors&ust=1752365868591001&usg=AOvVaw1lbZzmwnT_W7Cj3smd2sLt)\
  \ ([interactive tutorial](https://www.google.com/url?q=http://neuronpedia.org/gemma-scope&sa=D&source=editors&ust=1752365868591060&usg=AOvVaw2zB7uuUZ2h08pxEHu2K3Rh))\n\
  \nRelevant online communities: \n* [Open Source Mechanistic Interpretability Slack](https://www.google.com/url?q=http://neelnanda.io/osmi-slack-invite&sa=D&source=editors&ust=1752365868591225&usg=AOvVaw3VWPEPkS2YvtTgHg2fCz7a)\n\
  * [Mechanistic Interpretability Discord](https://www.google.com/url?q=https://discord.gg/ysVfhCfCKw&sa=D&source=editors&ust=1752365868591309&usg=AOvVaw3yZjyeaWa_bIyNgU6dfbEL)\n\
  * [Eleuther Discord](https://www.google.com/url?q=https://discord.gg/nHS4YxmfeM&sa=D&source=editors&ust=1752365868591381&usg=AOvVaw2ERuDkYfDt17xSVAv8vCnD)"
>>>>>>> 13b2c72 (x)
