content: "\uFEFFWorkshop Goals\nThe mechanistic interpretability field benefits from\
  \ a rich diversity of approaches\u2014from rigorous mathematical analysis to large-scale\
  \ empirical studies, from reverse-engineering a model via bottom-up circuit analysis,\
  \ to assisting behavioral analysis via top-down analysis of model representations.\
  \ But all are unified by the belief that there is meaning and structure to be found\
  \ inside neural networks, and that this is worth studying.\n\n\nThis diversity reflects\
  \ the field's breadth and the many valid paths toward understanding neural networks.\
  \ But those in these different sub-communities often lack natural venues to meet.\
  \ Our workshop aims to:\n* Showcase cutting-edge research across all approaches\
  \ to mechanistic interpretability\n* Foster cross-pollination between different\
  \ methodological traditions\n* Identify convergent insights emerging from diverse\
  \ research programs\n* Build understanding between different perspectives, research\
  \ communities, and terminologies\n* Welcome newcomers by providing clear entry points\
  \ into the field\n\n\nWe hope to explore points of active debate in the field including:\n\
  * How researchers should prioritise between gathering evidence via rigorous qualitative\
  \ analysis vs performance on benchmarks/real-world tasks\n* What are the implications\
  \ of new paradigms like reasoning models for the field\u2019s priorities?\n* Whether\
  \ the field\u2019s north star should be complete reverse engineering, achieving\
  \ high-level understanding, or something else entirely\n* How reliable or useful\
  \ are popular methods such as sparse autoencoders, and how much should we prioritize\
  \ them compared to other research directions?\n* The relative merits of a perspective\
  \ of curiosity driven basic science vs working towards specific goals\n* How important\
  \ are unsupervised techniques with the potential to surprise us, such as transcoders,\
  \ compared to simple supervised techniques such as probing.\n\n\nIn this workshop,\
  \ we hope to bring together researchers from across these many perspectives and\
  \ communities\u2014along with skeptics, experts in adjacent fields, and those simply\
  \ curious to learn more\u2014to facilitate healthy discussion and move towards a\
  \ greater mutual understanding as a field.\n\n\nThrough our call for papers, we\
  \ hope to facilitate the sharing of work in this fast-moving field, across all of\
  \ these axes, and especially work that helps to bridge these gaps. We welcome any\
  \ submissions that seek to further our ability to use the internals of models to\
  \ achieve understanding, regardless of how unconventional the approach may be. Please\
  \ see the call for papers page for further details and particular topics of interest.\n\
  \n\nWe welcome attendees from all backgrounds, regardless of your prior research\
  \ experience or if you have work published at this workshop. Note that while you\
  \ do not need to be registered for the NeurIPS main conference to attend this workshop,\
  \ you do need to be registered for the NeurIPS workshop track. No further registration\
  \ needed, seating is first-come first-served.\nLearning More\nHere are some resources\
  \ you may find useful for learning more about the mechanistic interpretability field\
  \ and performing research:\n* We recommend starting with the review paper Open Problems\
  \ in Mechanistic Interpretability for an overview of the field\n* Ferrando et al\
  \ is a good primer on the key techniques of the field\n* The ARENA coding tutorials\
  \ are a great place to learn how to implement these techniques in practice\n* Neel\
  \ Nanda\u2019s guide on how to skill up, learn how to do research, and pursue a\
  \ career as a researcher in the field\n\n\nResources for doing research\n* Popular\
  \ libraries include: TransformerLens (PyTorch, best for <=9B models), nnsight (PyTorch,\
  \ more flexible and scales better), Penzai (Jax)\n* The Mechanistic Interpretability\
  \ Benchmark\n* The Gemma Scope Sparse Autoencoders (interactive tutorial)\n\n\n\
  Relevant online communities:\n* Open Source Mechanistic Interpretability Slack\n\
  * Mechanistic Interpretability Discord\n* Eleuther Discord"
