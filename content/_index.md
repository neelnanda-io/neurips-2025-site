---
date: '2025-07-07'
draft: false
title: main
---

# **The Workshop**

This is a 1 day workshop at NeurIPS on mechanistic interpretability, held on December 6 or 7 at NeurIPS 2025 in San Diego, California. We invite submissions of short (4-pages) and long (9-pages) papers outlining new research in mechanistic interpretability, due August 22nd 2025 AoE.

Even though ever larger and more capable machine learning models are being deployed in real-world settings, we still know concerningly little about how they implement their many impressive capabilities. This in turn can make it difficult to rely on these models in high-stakes situations, or to reason about or address cases where said models exhibit undesirable behavior.

One emerging approach for understanding the internals of neural networks is mechanistic interpretability: reverse engineering the algorithms implemented by neural networks into human-understandable mechanisms, often by examining the weights and activations of neural networks to identify circuits that implement particular behaviors.

## **Why This Workshop?**

Mechanistic interpretability is a rapidly-growing topic of very broad interest, and one in which important breakthroughs have recently occurred. Recent years have seen a surge in academic labs and dedicated industry teams (e.g., Google DeepMind, Anthropic, OpenAI) focusing on MI, alongside commercial applications like the $50M startup Goodfire. The ICML 2024 workshop was the first of its kind and was a huge success - filling the allotted room to capacity.

## **Key Topics for NeurIPS 2025**

### **Sparse Autoencoders**

There have been substantial advances in mechanistic interpretability research since ICML 2024. The rise of sparse autoencoders (SAEs) for understanding neural networks has led to architectural diversification as well as an understanding of their flaws. These flaws, such as feature absorption and stochasticity between training runs, have motivated an interest in more theoretically grounded approaches and new methods.

### **Rigorous Benchmarking**

Another key area we want to highlight is the importance of benchmarking, to both reinforce the need for researchers to rigorously evaluate their work, and to share recent advances in benchmarks they could apply to their own work. There have been significant strides on comprehensive interpretability benchmarks, and on directly measuring the understanding achieved by interpretability.

### **Diverse Methods**

Sparse autoencoders are a popular approach to mechanistic interpretability, but far from the only viable one. In this workshop we seek to bring together researchers interested in SAE methodologies, those based in the theory of causal inference and abstraction, as well as concept-based interpretability.

## **Building the Community**

The mechanistic interpretability field is notable in that it has significant communities in academia, industry, and independent research. Historically, these communities have struggled to communicate and use different terminology, leading to duplicated work and hindering progress. Our workshop provides a common venue for these communities to interact and network, especially during poster sessions and breaks.

# **Speakers and Panelists**

## **Keynote Speakers**

### **Chris Olah**

**Affiliation:** Anthropic (interpretability lead and co-founder)

Chris Olah is a founder of the field of mechanistic interpretability and currently leads interpretability research at Anthropic.

### **Been Kim**

**Affiliation:** Google DeepMind

Been Kim is a researcher at Google DeepMind who has spent more than 15 years doing interpretability work.

### **Sarah Schwettmann**

**Affiliation:** Co-founder of Transluce

Sarah Schwettmann is a co-founder of Transluce, working on making neural networks more interpretable.

## **Panelists**

### **Naomi Saphra**

**Affiliation:** Harvard University

### **Atticus Geiger**

**Affiliation:** Pr(Ai)²R Group

### **Stella Biderman**

**Affiliation:** EleutherAI

# **Organizing Committee**

## **Neel Nanda (Contact Person)**

**Affiliation:** Senior Research Scientist, Google DeepMind**Email:** neurips2025@mechinterpworkshop.com

Neel Nanda is a Senior Research Scientist at Google DeepMind where he leads the mechanistic interpretability team. His team's recent work has focused on sparse autoencoders, introducing the state of the art JumpReLU architecture, and the widely used Gemma Scope suite of open source SAEs.

## **Martin Wattenberg**

**Affiliation:** Professor, Harvard University & Research Scientist, Google DeepMind

Martin Wattenberg is a professor of computer science at Harvard University and a research scientist at Google DeepMind. His expertise encompasses AI interpretability, human-AI interaction, and data visualization.

## **Sarah Wiegreffe**

**Affiliation:** Assistant Professor, University of Maryland & Postdoc, Allen Institute for AI

Sarah Wiegreffe is an incoming assistant professor at the University of Maryland and a current postdoc at the Allen Institute for AI (Ai2) and the University of Washington. She has worked on the explainability and interpretability of neural networks for NLP since 2017.

## **Atticus Geiger**

**Affiliation:** Lead, Pr(Ai)²R Group

Atticus finished his PhD thesis Uncovering and Inducing Causal Structure in Deep Learning Models at Stanford in 2023. Since then, he has led the Pr(Ai)2R Group, a small non-profit mechanistic interpretability research lab.

## **Julius Adebayo**

**Affiliation:** Founder and Researcher, Guide Labs

Julius Adebayo is a founder and researcher at Guide Labs. He is interested in building large-scale models that are engineered to be modular, steerable, and easy for humans to understand by enforcing interpretability constraints as part of the learning process.

## **Kayo Yin**

**Affiliation:** 3rd year PhD student, UC Berkeley

Kayo Yin is a 3rd year PhD student at UC Berkeley. She works on LLM interpretability and NLP for signed languages. Her research has been recognized by the ACL 2023 Best Resource Paper award and other honors.

## **Fazl Barez**

**Affiliation:** Senior Research Fellow, University of Oxford

Fazl Barez is a Senior Research Fellow at University of Oxford, where he works on topics related to Interpretability, AI safety, policy and governance.

## **Lawrence Chan**

**Affiliation:** Researcher, METR

Lawrence Chan is a researcher working on understanding the behavior of language models by either their internal components or via extensive human evaluations.

