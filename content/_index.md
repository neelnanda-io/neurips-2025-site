---
title: main
---

# **The Workshop 1**

This is a 1 day workshop at NeurIPS on mechanistic interpretability, held on December 6 or 7 at NeurIPS 2025 in San Diego, California. We invite submissions of short (4-pages) and long (9-pages) papers outlining new research in mechanistic interpretability, due August 22nd 2025 AoE.

Even though ever larger and more capable machine learning models are being deployed in real-world settings, we still know concerningly little about how they implement their many impressive capabilities. This in turn can make it difficult to rely on these models in high-stakes situations, or to reason about or address cases where said models exhibit undesirable behavior.

One emerging approach for understanding the internals of neural networks is mechanistic interpretability: reverse engineering the algorithms implemented by neural networks into human-understandable mechanisms, often by examining the weights and activations of neural networks to identify circuits that implement particular behaviors.

## **Why This Workshop?**

Mechanistic interpretability is a rapidly-growing topic of very broad interest, and one in which important breakthroughs have recently occurred. Recent years have seen a surge in academic labs and dedicated industry teams (e.g., Google DeepMind, Anthropic, OpenAI) focusing on MI, alongside commercial applications like the $50M startup Goodfire. The ICML 2024 workshop was the first of its kind and was a huge success - filling the allotted room to capacity.

## **Key Topics for NeurIPS 2025**

### **Sparse Autoencoders**

There have been substantial advances in mechanistic interpretability research since ICML 2024. The rise of sparse autoencoders (SAEs) for understanding neural networks has led to architectural diversification as well as an understanding of their flaws. These flaws, such as feature absorption and stochasticity between training runs, have motivated an interest in more theoretically grounded approaches and new methods.

### **Rigorous Benchmarking**

Another key area we want to highlight is the importance of benchmarking, to both reinforce the need for researchers to rigorously evaluate their work, and to share recent advances in benchmarks they could apply to their own work. There have been significant strides on comprehensive interpretability benchmarks, and on directly measuring the understanding achieved by interpretability.

### **Diverse Methods**

Sparse autoencoders are a popular approach to mechanistic interpretability, but far from the only viable one. In this workshop we seek to bring together researchers interested in SAE methodologies, those based in the theory of causal inference and abstraction, as well as concept-based interpretability.

## **Building the Community**

The mechanistic interpretability field is notable in that it has significant communities in academia, industry, and independent research. Historically, these communities have struggled to communicate and use different terminology, leading to duplicated work and hindering progress. Our workshop provides a common venue for these communities to interact and network, especially during poster sessions and breaks.

