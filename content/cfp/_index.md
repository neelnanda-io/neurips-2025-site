# Call for Papers

We are inviting submissions of short (max 4 pages) and long (max 9 pages) papers outlining new research, due August 22, 2025. We are extremely grateful to all who volunteer as reviewers, you can [express interest here](https://www.google.com/url?q=https://docs.google.com/forms/d/e/1FAIpQLSdiw1SJllzoTz_nqzDTzTOGb9DV3W_truQyh-WvYj_QGIi7Mg/viewform?usp%3Ddialog&sa=D&source=editors&ust=1752120875701096&usg=AOvVaw2Op6T6anuEMwpaBafzD12W).


Details:


* The workshop is non-archival.

* Authors will be notified of acceptance by September 19

* We do not accept submissions of work that has been accepted to an archival venue.

* Submissions undergoing peer review (on August 22) are welcome, including works under review at the main NeurIPS conference.

* All submissions must be made via OpenReview (link forthcoming).

* Note: If you do not have an institutional email, be aware that it can take up to 2 weeks to get an OpenReview account approved. [Let us know](mailto:neurips2025@mechinterpworkshop.com) if this prevents you from submitting.

* Please use the [NeurIPS 2025 LaTeX Template](https://www.google.com/url?q=https://media.neurips.cc/Conferences/NeurIPS2025/Styles.zip&sa=D&source=editors&ust=1752120875702281&usg=AOvVaw2-q9UAfpzGXAh_5ABAnKkj) for all submissions (no need for a checklist).

* Both short (max 4 page) and long (max 9 page) papers allow unlimited pages for references and appendices, but reviewers are not expected to read these.

* Accepted papers will be allowed one additional page in the camera ready version, to integrate reviewer feedback.

* Long works will be held to a higher standard of rigour and depth than short works.

* We welcome any work that furthers the field of mechanistic interpretability, even if in unconventional ways. This includes:

* Rigorous negative results

* Rigorous replications of important results

* Open source software and tools (e.g. [TransformerLens](https://www.google.com/url?q=https://github.com/neelnanda-io/TransformerLens&sa=D&source=editors&ust=1752120875703144&usg=AOvVaw2f31YTxSZlrhpiCiz6nyWh), [Neuronpedia](https://www.google.com/url?q=http://neuronpedia.org&sa=D&source=editors&ust=1752120875703233&usg=AOvVaw372m91yMKUvHqSozWYZUpT), [nnsight](https://www.google.com/url?q=https://github.com/ndif-team/nnsight&sa=D&source=editors&ust=1752120875703353&usg=AOvVaw0X7IeuWHbkKH6hv4B2t2uW) or [Penzai](https://www.google.com/url?q=https://github.com/google-deepmind/penzai&sa=D&source=editors&ust=1752120875703461&usg=AOvVaw2NZdFgyvwTlqus40HNtznD))

* Models or datasets that may be of value to the community (e.g. [Pythia](https://www.google.com/url?q=https://arxiv.org/abs/2304.01373&sa=D&source=editors&ust=1752120875703661&usg=AOvVaw0hjFSZSyPVpxWd-Jo0Iyqr), [MultiBERTs](https://www.google.com/url?q=https://arxiv.org/abs/2106.16163&sa=D&source=editors&ust=1752120875703729&usg=AOvVaw0VVQ28ScdtUOPCPfUAABfU) or [Gemma Scope](https://www.google.com/url?q=https://arxiv.org/abs/2408.05147&sa=D&source=editors&ust=1752120875703791&usg=AOvVaw1ZA7gAhwcIHP0GUyLQsql0))

* Educational materials (e.g. [the ARENA materials](https://www.google.com/url?q=https://arena3-chapter1-transformer-interp.streamlit.app/&sa=D&source=editors&ust=1752120875703932&usg=AOvVaw2Yn60dZMs1Pa1dQ5Xr6D6w))

* [Distillations](https://www.google.com/url?q=https://distill.pub/2017/research-debt/&sa=D&source=editors&ust=1752120875704021&usg=AOvVaw0EsVjmSz5jBZFTtzkOUNhN) of key and poorly explained concepts (e.g. [Ferrando et al](https://www.google.com/url?q=https://arxiv.org/abs/2405.00208&sa=D&source=editors&ust=1752120875704175&usg=AOvVaw2b6PLkeQAYmKCKLJVkB1a-))

* Position pieces that bring clarity to complex topics and debates (e.g. [the ‘strong’ feature hypothesis could be wrong](https://www.google.com/url?q=https://www.alignmentforum.org/posts/tojtPCCRpKLSHBdpn/the-strong-feature-hypothesis-could-be-wrong&sa=D&source=editors&ust=1752120875704525&usg=AOvVaw3hRmew_cInoxIY258qABaJ))

### Topics of Interest

We welcome submissions on any topic that furthers our understanding of models by leveraging their internal states. The field is young, and there are many exciting open questions. We are particularly interested in, but not limited to, the following directions:


* Model Biology & Cognition

* What can we understand about the high-level properties of models? Can we find evidence for cognitive phenomena like [implicit planning](https://www.google.com/url?q=https://transformer-circuits.pub/2025/attribution-graphs/biology.html%23dives-poems&sa=D&source=editors&ust=1752120875705377&usg=AOvVaw3-KDLFhdK9FjH6iGnpXfVe), search algorithms, or [internal world models](https://www.google.com/url?q=https://arxiv.org/abs/2210.13382&sa=D&source=editors&ust=1752120875705481&usg=AOvVaw0R4N1ZFMxYjjZsFgSynG8K)?

* What does it look like for a model to "believe" something? Can we find and manipulate these beliefs? Or is the entire notion of a model’s beliefs confused?

* Do models internally represent different [personas](https://www.google.com/url?q=https://arxiv.org/abs/2406.12094&sa=D&source=editors&ust=1752120875705885&usg=AOvVaw35geFGdwu9YtdSPhkztC1u) or [simulators](https://www.google.com/url?q=https://www.nature.com/articles/s41586-023-06647-8&sa=D&source=editors&ust=1752120875705986&usg=AOvVaw2SnZDm2uF6wYSFsI4P6-yb) that drive their behavior, and how are these selected from context?

* Circuits and Causal Analysis

* [Circuit analysis](https://www.google.com/url?q=https://distill.pub/2020/circuits/zoom-in/&sa=D&source=editors&ust=1752120875706249&usg=AOvVaw3AJFqWcqrpmBi_b0wwXPJm) is a core part of mech interp, but our methods are still nascent. What are the best ways to find and validate circuits?

* How can we improve existing approaches like [attribution](https://www.google.com/url?q=https://arxiv.org/abs/2406.11944&sa=D&source=editors&ust=1752120875706530&usg=AOvVaw23WNeThpzRidyUQhYS_Ym1)[graphs](https://www.google.com/url?q=https://transformer-circuits.pub/2025/attribution-graphs/methods.html&sa=D&source=editors&ust=1752120875706613&usg=AOvVaw3N10fcXUqEd170hjJ0a00a)?

* What can we learn from [the field of causal inference](https://www.google.com/url?q=https://arxiv.org/abs/2407.04690&sa=D&source=editors&ust=1752120875706828&usg=AOvVaw0nme4aHhMhWKLlKGjJlCWR) to make our analysis more rigorous?

* What are the [failure modes](https://www.google.com/url?q=https://arxiv.org/abs/2307.15771&sa=D&source=editors&ust=1752120875707001&usg=AOvVaw2a1V7E7_pHH00y_yd_8JGv) of current causal methods, and what alternative approaches might bear fruit?

* How far can we push [weight-based](https://www.google.com/url?q=https://arxiv.org/abs/2301.05217&sa=D&source=editors&ust=1752120875707264&usg=AOvVaw0-ncr2Cs1t9kCnN2WAQgRo)[analysis](https://www.google.com/url?q=https://arxiv.org/abs/2410.08417&sa=D&source=editors&ust=1752120875707349&usg=AOvVaw0n5sivhLy-ypXH6Ci5Ceh0)?

* Unsupervised Discovery & Dictionary Learning

* A key promise of interpretability is its potential to surprise us by revealing unexpected structure. How well do unsupervised methods like [sparse](https://www.google.com/url?q=https://arxiv.org/abs/2103.15949&sa=D&source=editors&ust=1752120875707780&usg=AOvVaw2VP1hyFSEPiFenLC-H2Dni)[autoencoders](https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features&sa=D&source=editors&ust=1752120875707897&usg=AOvVaw0olIIX_Z-K-n3EWOwl5J7N), [patch](https://www.google.com/url?q=https://arxiv.org/abs/2401.06102&sa=D&source=editors&ust=1752120875707988&usg=AOvVaw3Oef5Kfm6qAnYo9MnkfrD_)[scopes](https://www.google.com/url?q=https://arxiv.org/abs/2403.10949v2&sa=D&source=editors&ust=1752120875708053&usg=AOvVaw3XS9xmUq9NkUmTfbtDg6a4), or training [data](https://www.google.com/url?q=https://arxiv.org/abs/2308.03296&sa=D&source=editors&ust=1752120875708148&usg=AOvVaw0usB74qvhkQlpE81GvzE6p)[attribution](https://www.google.com/url?q=https://arxiv.org/abs/2205.11482&sa=D&source=editors&ust=1752120875708239&usg=AOvVaw2lM77YDLIJa502X9AgI3_H) actually work for this?

* These methods have shown great promise, but their utility on downstream tasks remains an open question. We welcome work that rigorously tests their practical value against strong baselines.

* Practical Applications & Benchmarking

* If our interpretability tools are teaching us something real, they should be useful. How well do they perform on real-world downstream tasks against well-implemented baselines?

* How can we develop objective benchmarks that require genuine understanding to solve, such as eliciting secret knowledge or hidden goals?

* Can we move beyond proxies and find ways to objectively measure what we actually care about, like the "understanding" captured by a natural language hypothesis?

* Interpreting Reasoning & Chain of Thought

* Reasoning models are a big deal, we understand very little about them, and they introduce significant new challenges for interpretability.

* When is a model's Chain of Thought a faithful representation of its computation, and when is it post-hoc rationalization? What role does it actually play in the model's final output?

* How might we interpret latent reasoning models that replace transparent text-based thoughts with opaque vectors?

* Auditing & Evaluating Safety-Relevant Behaviors

* Recent work has shown sophisticated and sometimes concerning behaviors in models, like self-preservation or alignment faking. What's really going on here?

* Is this just anthropomorphism, or can interpretability tools help us find something genuinely concerning?

* Debugging & Fixing Models

* How can we apply the interpretability toolkit to understand and ideally fix unexpected model behaviors, from simple logical errors (e.g., thinking 9.8 < 9.11) to complex real-world problems like jailbreaks and hallucinations?

* Monitoring & Inference-Time Interventions

* How can we use interpretability techniques (probes, anomaly detection, SAEs) to monitor models in deployment?

* A key challenge is making these methods cheap, fast, and effective enough for production, especially in models with million-token context windows.

* Do these methods actually beat simple, strong baselines like just prompting another language model?

* Developmental Interpretability

* What can we learn about what happens during training? How and why do models and their circuits form the way they do?

* Scaling & Generalizing Interpretability

* How do findings and techniques from prior work on small models hold up at the frontier? What breaks and what scales?

* How do multimodal models represent, combine, and interfere with information from different modalities?

* What can we learn by interpreting alternative architectures like diffusion models, state space models, or graph neural networks?

* Conceptual & Foundational Work

* Our field is built on concepts like "features" and "circuits" that remain poorly defined. We welcome position papers that bring conceptual clarity and rigor to these foundational questions.

* Automating Interpretability

* How much of the interpretability research process can be automated using LLMs? How does automated analysis compare to human performance?

* Rigorous Case Studies

* We welcome rigorous, "biology-inspired" analyses of specific components or phenomena, especially those combining deep qualitative case studies with quantitative sanity checks and falsifiable hypotheses.

* Open Source & Tooling

* We strongly encourage submissions that provide open-source resources to accelerate the research of others, from better tooling and datasets to trained interpretability models (e.g., SAEs).

We enthusiastically welcome both positive and negative results—the goal is to learn. Distillations of key-but-confusing concepts and high-quality educational materials are also highly valued.